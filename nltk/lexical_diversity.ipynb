{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK application - lexical diversity\n",
    "\n",
    "__Reading material:__\n",
    "- [NLTK Book](http://www.nltk.org/book/ch01.html) 1.3, 1.4, 3.1, 3.6, 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at [this](https://www.theguardian.com/books/2009/apr/03/agatha-christie-alzheimers-research) article about declining metrics of language use in Agatha Christie’s later books. Is it possible to detect Alzheimer’s or other mental conditions from a person’s writing? We’re not going to answer that question today, but natural language processing is a very powerful and promising field!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Before you begin, execute the command <tt>nltk.download()</tt>. Select the “All Packages” tab and use it to download <tt>averaged_perceptron_tagger</tt>, <tt>maxent_treebank_pos_tagger</tt>, <tt>punkt</tt>, and <tt>wordnet</tt>. If at any point in this notebook you get a LookupError like “Resource 'XXXXX' not found. Please use the NLTK Downloader to obtain the resource”, find the resource in the downloader “All Packages” tab, download it, and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Use the instructions from [3.1](http://www.nltk.org/book/ch03.html) of the NLTK book to have Python download the text of [The Mysterious Affair at Styles](http://www.gutenberg.org/files/863/863-0.txt) from Project Gutenberg. In Python 2.7, the <tt>urlopen</tt> function is located directly in the <tt>urllib</tt> package; not <tt>urllib.request</tt>. Be sure to call the decode <tt>(‘utf8’)</tt> (as in the example) after reading from the URL; this saves the data as a Unicode string (hence you may see a u before every string), which NLTK understands better than the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url = \"http://www.gutenberg.org/files/863/863-0.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.There’s some text before the book begins and after it ends that we don’t want to include in the analysis. Chapter [3.1](http://www.nltk.org/book/ch03.html) of the NLTK book suggests that you use the string’s <tt>find</tt> and/or <tt>rfind</tt> methods to look for phrases that mark the beginning and ending of the text. \n",
    "\n",
    "Find the index corresponding with the beginning of the last (not first, as suggested in the example) occurrence of the string “CHAPTER I. I GO TO STYLES”, and find the index corresponding with the end of the last occurrence of the string “THE END”. Use these indices to discard data before and after these markers. Print the beginning and end of the leftover string to confirm correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.rfind(\"CHAPTER I. I GO TO STYLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i0 = raw.rfind(\"CHAPTER I. I GO TO STYLES\")\n",
    "print(raw[i0:i0+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = raw.rfind(\"THE END\")\n",
    "print(raw[i1:i1+7])\n",
    "i1 = i1+6 # this is really the index we want\n",
    "print(raw[i1:i1-7:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = raw[i0:i1+1]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0:10])\n",
    "print(text[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Use NLTK's <tt>FreqDist</tt> function to generate a dictionary of the words in the book and the frequency of each. Calculate the “lexical diversity” as defined in [1.4](http://www.nltk.org/book/ch01.html) of the NLTK book, except that you can use the <tt>FreqDist</tt> object you just made rather than a new set to count unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()\n",
    "d = nltk.FreqDist(tokens)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()\n",
    "d = nltk.FreqDist(tokens)\n",
    "lexical_diversity = float(len(d))/len(tokens)\n",
    "print(\"The ratio of unique tokens to total tokens is:\", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.We noticed that the <tt>split()</tt> method left punctuation marks at the end of words, counting these as distinct from the word itself. Improve your frequency distribution dictionary by using NLTK’s <tt>word_tokenize</tt> function instead of <tt>split</tt> to separate the words into a list. Also, before you do that, get rid the original text of underscores (<tt>\"__\"</tt>), which are used here to indicate emphasis, as these shouldn’t make the enclosed word distinct. \n",
    "\n",
    "Calculate the lexical diversity from this FreqDist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"hello, world!!!\"\n",
    "print(s.split())\n",
    "print(nltk.word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"_\",\"\")\n",
    "tokens = nltk.word_tokenize(text)\n",
    "d = nltk.FreqDist(tokens)\n",
    "lexical_diversity = float(len(d))/len(tokens)\n",
    "print(\"The ratio of unique tokens to total tokens is:\", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.It doesn’t really make sense to count “Nature” separately from “nature”, or “Nature” separately from “Natural” or “Naturally” if we’re trying to get a sense of vocabulary. \n",
    "\n",
    "Improve your FreqDist by eliminating words that are distinct only in letter case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"abc\".upper())\n",
    "print(\"ABC\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = text.lower()\n",
    "tokens2 = nltk.word_tokenize(lower)\n",
    "d2 = nltk.FreqDist(tokens2)\n",
    "lexical_diversity = float(len(d2))/len(tokens2)\n",
    "print(\"The ratio of unique tokens to total tokens is:\", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Also eliminate those that differ only in affixes (as described in 3.6. You’re welcome to try either of the <tt>stemmers</tt> or the WordNet <tt>lemmatizer</tt> for this. The stemmer is probably more accurate, because the lemmatizer does not reduce all words to their lemma, but the downside is that the stemmers returns <tt>stems</tt> which may not be words at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = nltk.PorterStemmer()\n",
    "print(p.stem(\"running\"))\n",
    "print(p.stem(\"runs\"))\n",
    "print(p.stem(\"runned\")) # The stemmer uses simple rules to try to extract the \"stem\".\n",
    "print(p.stem(\"ran\"))   # It's not perfect because English is so irregular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = map(p.stem,tokens2)\n",
    "d3 = nltk.FreqDist(stems)\n",
    "lexical_diversity = float(len(d3))/len(tokens2)\n",
    "print(\"The ratio of unique word stems to total tokens is:\", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Use the most_common method of the FreqDist from step 6 (not step 7, because that FreqDist might be made up of stems rather than words) to print out the most-used 30 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Oops, even after all that, some of the common “words” are actually punctuation marks.  We could remove those from the dictionary, but let’s leave the dictionary alone and instead just show the top 30 words that begin with a letter.\n",
    "\n",
    "Turn the dictionary-like FreqDist object into a list of word/frequency (key/value) pairs, sort the list by the word frequency numerically (not by word alphabetically) in descending order, and print out the 30 most commonly used words that begin with a letter and their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"abc\".isalpha())\n",
    "print(\"!!a\".isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(d2[key],key) for key in d2 if key[0].isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [(d2[key],key) for key in d2 if key[0].isalpha()]\n",
    "l = sorted(l, reverse = True)\n",
    "for pair in l[:30]:\n",
    "    print(pair[1], \":\", pair[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.Some words like “point” are used in several different senses of the word, and intuitively we should somehow factor this into the concept of lexical diversity. Turn the <tt>word_tokenized</tt> list from step 6 into an <tt>nltk.Text</tt> object (3.1) and use the concordance function (1.3) on “point” to see what I mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.concordance(\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.[WordNet](http://www.nltk.org/howto/wordnet.html) maintains a structure of “synsets”, which are like distinct concepts or meanings of words. It would be nice if we could determine the WordNet <tt>synset</tt> with which each occurrence of the word corresponds; that way we could consider the different WordNet <tt>synset</tt> in our lexical diversity calculation. The [Lesk algorithm](http://www.nltk.org/howto/wsd.html) attempts to do just that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Print all the different WordNet <tt>synset</tt> definitions for the word “point”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_word = \"point\"\n",
    "text = nltk.Text(tokens2)\n",
    "text.concordance(chosen_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = wn.synsets(chosen_word)\n",
    "for i in x:\n",
    "    print(i, i.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) The Lesk algorithm needs the sentence the word appears in to determine the corresponding synset. \n",
    "\n",
    "Write a function that returns a list of all the sentences in the original text in which the word appears. \n",
    "\n",
    "Print all the sentences in which the word “point” appears. [Chapter 3.8](http://www.nltk.org/book/ch03.html) of the book will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(word,text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return [s for s in sentences if word in nltk.word_tokenize(s)]\n",
    "\n",
    "text = raw[i0:i1+1]\n",
    "sentences = get_sentences(chosen_word,text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use the <tt>lesk</tt> function to determine the synset definition and part of speech of “point” in each sentence. (It does very poorly.)\n",
    "\n",
    "Write a function that attempts to disambiguate the sense of the word in \n",
    "a sentence using the Lesk algorithm, then return the definition of that\n",
    "sense of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lesk(word,sentence):\n",
    "    syn = lesk(nltk.word_tokenize(sentence),word)\n",
    "    return syn.definition(),syn.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    definition,pos = my_lesk(chosen_word,sentence)\n",
    "    print(\"The definition of point, as used in this sentence, is:\")\n",
    "    print(definition)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Maybe if we tag the parts of speech first using the <tt>pos_tag</tt> function [(Chapter 5)](http://www.nltk.org/book/ch05.html) and provide that information as the (optional) second argument to the <tt>lesk</tt> function, it will do a better job?\n",
    "\n",
    "Note that <tt>pos_tag</tt> doesn’t provide the part of speech for just a single word, and you’ll have to convert between the output provided by the part of speech tagger and a single letter representing the part of speech required by <tt>lesk</tt>. This doesn’t have to work for all possible parts of speech, just the parts of speech of “point” in the text.\n",
    "This still does poorly, but it was a good idea. It seems like the <tt>Lesk</tt> algorithm just tries to match up words from the context sentence with words from the synset definition, which is probably too short for this approach to work well. There’s certainly a lot of work left to be done in the field of natural language processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lesk2(word,sentence):\n",
    "    sen_tokens = nltk.word_tokenize(sentence)\n",
    "    sen_tagged = nltk.pos_tag(sen_tokens)\n",
    "    ind = sen_tokens.index(word)\n",
    "    pos = sen_tagged[ind][1]\n",
    "    syn = lesk(sen_tokens,word,pos= pos[0].lower())\n",
    "    return syn.definition(),pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in get_sentences(chosen_word, raw):\n",
    "    print(\"Word is:\",chosen_word)   \n",
    "    print(\"As in:\", sentence)\n",
    "    meaning, pos =  my_lesk(chosen_word,sentence)\n",
    "    print(\"Without POS:\", meaning)\n",
    "    print(\"Assumed POS:\", pos)\n",
    "    meaning, pos =  my_lesk2(chosen_word,sentence)\n",
    "    print(\"With POS:\",meaning)\n",
    "    print(\"POS:\",pos)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
